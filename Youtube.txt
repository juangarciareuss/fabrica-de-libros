Xavier Mitjana
Nanobanana ha sido una revolución. Es la mejor herramienta para generar, editar y transformar imágenes. Y además la puedes utilizar de un modo completamente gratuito. Su potencial es enorme, por lo
que para aprender a sacarle provecho, lo mejor es ver ejemplos prácticos. Y en el vídeo de hoy no te traigo ni uno, ni dos ni tres. Te traigo hasta 30 casos de uso
prácticos para que puedas inspirarte y sacarle el máximo partido a esta herramienta. Desde lo más básico a ejemplos avanzados. Dentro vídeo. Hoy vamos a ver 30 casos
de uso de nanobanana que vosotros podéis replicar de modo gratuito en Google I Studio. Simplemente tenéis que tener una cuenta de Google, entrar a esta dirección de aquíudio.google.com
y aseguraros que empezáis a chatear con el modelo Gemini 2.5 Flash Image Preview activado. Y nada más. Con esto ya podréis hacer todo lo que os voy a
mostrar en este vídeo. Y empezamos ya directamente con los tres primeros casos de uso, que son casos de uso un poco más sencillos, pero que seguramente le encontraréis utilidad inmediata casi
todos. El primero es el de mejora de imágenes. Y es que con nanobanana, utilizándolo dentro de Google Studio, por ejemplo, aunque también lo podéis hacer dentro de Gémini, le podemos pedir
cosas como estas. corrige el balance de blancos de esta foto y su exposición. Y podemos ver como si le proporcionamos una foto donde está muy contrastada y se
ve un poco naranjosa, automáticamente Nanobanana es capaz de convertirla en una imagen como esta, una imagen donde ha corregido por completo el balance de blancos y donde ha recuperado un poco
las sombras. Pues el primer caso de uso sería este, simplemente revelar mejor imágenes que se han tomado mal. Por ejemplo, si utilizáis una cámara fotográfica en modo manual. Siguiente
caso de uso, ¿cuál es? Pues otro también bastante habitual y que reemplaza una tarea de Photoshop que es bastante tediosa, como es la corrección de pieles. Sí que es verdad que el
resultado igual es un poco excesivo, pero fijaros que lo hace bastante bastante bien. Aquí podemos ver la imagen de referencia donde vemos una chica con bastantes marcas de acné en la
cara y si a nanobanana le pedimos simplemente que elimine el acné y las marcas de piel de esta fotografía, automáticamente nos devuelve una imagen como esta de aquí. El texturizado de la
piel quizá tenga un suavizado excesivo, pero realmente ha mantenido la consistencia total de la persona de referencia, por lo que este sería un segundo caso de uso. Vamos allá con el
tercero. El tercer caso de uso sería cuando tenemos imágenes rotas o que se deben recomponer. Por ejemplo, una imagen como esta donde se ve que la chica ha tomado la foto a partir de un
espejo roto. podemos ver como nanobanana es capaz de interpretar todos estos fragmentos y reconstruir la imagen perfectamente. Con esto ya tendríamos un tercer caso de
uso, pero hay más. Imaginémonos ahora que queremos trabajar con objetos o que tenemos un e-commerce. Los siguientes casos de uso son especialmente interesantes. El primero, el primer caso
de uso sería el de poder poner o quitar objetos. En este caso empezamos por quitar un objeto. En este caso tenemos la imagen de este chico con estas gafas
de sol. Y imaginaros que lo que queremos es simplemente quitárselas, pues simplemente pidiéndoselo. Nanobanana es muy bueno haciendo esto, quitando
elementos de una imagen. Esto se puede aplicar eh simplemente para quitar objetos, pero podemos hacer cosas un poco más elaboradas. Por ejemplo, imaginaos que tenemos esta foto de esta
modelo, que tenemos un producto que es algo más ochoentero y que no queremos hacer otra sesión de fotos, pues lo que podemos hacer es, por ejemplo, pedirle que le cambie el estilo, que le cambie
el peinado. Podemos conseguir una imagen como esta simplemente utilizando un prome. Quiero que edites esta imagen para ver a esta mujer con un peinado de
pelo corto rosa de inspiración 80. Y tal y como hemos visto, el resultado se ajusta muy muy bien a la a la indicación. Pues bien, una vez la tenemos con el estilo que nosotros
queremos, es cuando podríamos incorporar objetos. Aquí, en este caso, se lo voy a pedir sin referencia. Luego veremos cómo se puede trabajar con referencias múltiples. Y he continuado la conversación diciéndole esto. Ahora
quiero que edites esta imagen para que lleve unas gafas de sol acorde con la moda de los 80 y una chaqueta de cuero y automáticamente pues nano banana. lo ha
hecho y el resultado y la coherencia que mantiene entre las diferentes iteraciones, como podéis ver, es altísima, pero se pueden hacer más cosas con nanobanana. Si nos vamos al siguiente ejemplo, ya es cuando
trabajamos por primera vez con múltiples referencias. Aquí, ¿qué podemos hacer? Le podemos proporcionar la imagen de un producto. Imaginaros que tenemos un e-commerce y queremos vender este
vestido. Tenemos la imagen de una de nuestros modelos que está anunciando en este caso otro tipo de vestido, pero queremos que ahora generar una imagen donde vista este vestido. Pues
simplemente diciéndole algo como esto, quiero una foto para e-commerce donde la modela donde la modelo vista el vestido floral de la otra imagen, pues de un modo muy rápido con nanobanana
conseguimos un resultado como este que estáis viendo aquí. Y fijaros en un detalle, incluso ha intentado mantener el detalle del tatuaje. Aquí podemos ver
este tatuaje en el brazo derecho que también tiene en la imagen original. Aquí ya hemos empezado a trabajar con referencias múltiples y esto es realmente muy interesante, pero es que
con nano banana podemos hacer lo opuesto. ¿Y qué es lo opuesto? Pues nosotros podemos partir de una única imagen, como puede ser la imagen de un personaje que hayamos generado con inteligencia artificial y luego para
generar otras imágenes que mantengan la coherencia le podemos pedir que nos genere múltiples referencias como una hoja de modelo de personaje, una imagen como esta, donde lo que le hemos pedido
a la nueva es que cree una imagen model sheet con las cuatro vistas de este personaje frontal, perfil izquierdo, derecho y perfil derecho y izquierdo y la imagen trasera y que estén aisladas
en un fondo blanco. Y como podéis ver lo ha hecho perfectamente. Y la gracia es que con esto nos lo podemos llevar a otra conversación con Nano Banana y podemos generar diferentes imágenes de
este personaje. Por ejemplo, le subimos el model sheet que hemos generado en el paso anterior y le pedimos algo como esto. Quiero que uses esta model Sheet para generar imágenes de este personaje
en diferentes situaciones. La primera, quiero este personaje paseando por Nueva York desde un ángulo heroico contrapicado y con él andando con decisión. Toma tomada con una lente de gran angular y automáticamente es capaz
de generar la imagen que nosotros queremos. Pero acto seguido lo que podemos hacer es que nos genere una imagen desde atrás, una imagen como esta
de aquí. Y esto, ¿por qué es interesante? Es interesante porque podemos aplicarlo a otro caso de uso que me parece absolutamente alucinante de nanobanana, cómo es la posibilidad de
crear fotogramas clave. Por ejemplo, si utilizamos esta imagen como fotograma inicial y esta imagen como fotograma final, podemos crear un clip como este
que estáis viendo ahora mismo. Pero si lo complicamos un poco más, podemos llegar a crear escenas como esta. Vamos allá. Set. Casco y listo.
Esto va rapidísimo. Cada día es como si fuera cumpleaños. Atención, atención. Deténgase, deténgase. Téngase ahora mismo. Entregue
esa nueva inteligencia artificial. Es ilegal. Atención, deténgase, deténgase. Téngase ahora mismo. Es una orden. Deténgase. Entregue esa nueva donde los diferentes fotogramas clave se han ido
creando a partir de la interacción con Nano Banana. Y si esta secuencia os ha parecido interesante, atentos al próximo vídeo que voy a publicar, porque justo va a ser un tutorial sobre cómo crear este tipo de escenas. Escenas en vídeo
generadas con inteligencia artificial, con total continuidad, tan largas como queráis. Dicho esto, seguimos con los casos de uso que le podéis dar a Nanonoban dentro de Google i Studio. Y
es que siguiendo con el tema de las múltiples referencias, si vosotros cogéis y le mandáis varias fotos vuestras, en este caso fotos que tenía de un dataset para entrenar modelos,
ahora ya no hace falta que lo utilicéis para entrenar un modelo, sino que simplemente pasándole las múltiples referencias le podéis pedir algo como esto. crea una imagen de este hombre esperando el autobús en un día lluvioso
mientras lee el periódico en París de los años 60. Fotografía estilo bresón en blanco y negro. Y fijaros la imagen que genera. En este caso, el parecido es
bastante bastante razonable y un resultado que se acerca a haber entrenado un lora en otros modelos. De hecho, hecho esta referencia, esta imagen y también usando la misma
técnica, pasándole otra vez todas mis fotos, le he pedido esto. Crea una imagen espectacular de esta persona con iluminación dramática y composición heroica, montando un búfalo en un rodeo
en Texas. Y fijaros aquí cómo ha conseguido un parecido muy muy notable, aunque la cuerda esté un poco al aire y no haga nada, pero más allá de esto, la
imagen realmente está muy muy bien. Deberían corregirse estos detalles, pero lo importante que es el parecido que debe guardar conmigo, yo creo que lo ha respetado bastante bien. Por lo que otro
caso de uso que tiene Noanana es el sustituir los tediosos procesos de entrenamiento de modelos para generar imágenes consistentes de personajes. Con la nova lo podéis hacer simplemente
pasándole múltiples referencias de una misma persona. Y acto seguido vamos a centrarnos en un sector concreto como puede ser por ejemplo el sector inmobiliario. Porque aquí nanobanana
tiene un montón de casos de uso, desde limpiar imágenes, donde como podéis ver, yo le he mandado una imagen de una estancia que está completamente movada y simplemente pidiéndole que edite la
imagen y borre todos los muebles para dejar la estancia vacía, conseguimos algo como esto. Aún se podría quitar algún mueble extra, se ha dejado alguno, pero el resultado de entrada es muy
bueno y hacer esto con Photoshop nos llevaría horas. Si seguimos, otro ejemplo sería eh pedirle no que nos quite los muebles, sino que nos haga un home staging
virtual. Es decir, que de una estancia como esta que estáis viendo aquí, que es evidentemente una estancia muy antigua, con todos los muebles muy viejos, le podamos pedir algo como esto. Realiza un
home sting de esta estancia, respeta la estructura del espacio, mejora el acabado de las paredes y usa muebles modernos y automáticamente nos devuelve algo como esto. El ejemplo en este caso
es muy radical porque la imagen original era un espacio muy antiguo, pero aún así ha conseguido un resultado muy pero que muy decente. Y si no queréis
transformaciones tan radicales como quitar todos los muebles o que os haga un home staging entero, lo que podéis hacer es, por ejemplo, simplemente añadir objetos puntuales. En este caso,
partimos de una imagen como esta, donde una alfombra le iría bien. Y si le pedimos que ponga una alfombra de pelo corto y color amarillo debajo de la mesita, vemos cómo es capaz de hacerlo
con una precisión realmente asombrosa. Hace muy pocos meses esto solo se podía hacer utilizando herramientas como con fui. Y desde que han aparecido modelos
como este o como los modelos de flux Context, pues esto se ha simplificado un montón. Y si seguimos en el siguiente caso de uso, veréis como se puede incluso más fino. Es decir, en vez de
editar una imagen entera, podemos editar un elemento concreto, como puede ser, por ejemplo, este cuadro de aquí, que si minimizo veréis que se ve aquí y que es
un detalle pequeño de la imagen. Pues bien, si yo cojo esta imagen, se la subo y le digo algo como esto, reemplaza el cuadro grande del fondo por un cuadro estilo Joan Miró con toda su paleta de
colores habitual, fijaros qué es lo que nos devuelve. Nos devuelve la misma imagen, pero con este cuadro de aquí que efectivamente ha cogido la paleta de
colores y un poco el estilo de Joan Miró para sustituir el cuadro original, por lo que podéis eh escoger el nivel de intensidad de la edición. si os hace todo el trabajo por vosotros, es decir,
si os hace un home staging entero y lo cambia todo, si queréis que os añada objetos o incluso editar objetos concretos dentro de una imagen. En el siguiente caso de uso podemos ver
también otro matiz o otro tipo de edición que es muy interesante, como es los cambios de iluminación. Nosotros a una escena le podemos pedir que Nanobanana edite la imagen para que la
fotografía parezca sacada en otro momento del día. Por ejemplo, al atardecer podemos pasar de una imagen como esta a una imagen como esta de
aquí, de un modo muy rápido. Pero es más, también le podemos pedir tareas sencillas de edición como hacer reemplazos completos de fondo. tenemos esta imagen original de este perro y le
podemos pedir, por ejemplo, que en este caso lo que reemplace sea al fondo, que no toque ningún elemento concreto, sino que reemplace el fondo de esta imagen para que el perro parezca perseguido por
gatos en medio de la ciudad. Y como podéis ver, también lo ha hecho sin ningún tipo de problema. Las patas traseras del perro en este caso se ven escondidas debajo del pavimento, por lo
que deberíamos corregir este detalle, pero esencialmente esto sucede porque las patas del perro original se encontraban por debajo delespal y como podéis ver, por lo que simplemente ha
hecho un reemplazo de fondo. Y esto es muy interesante, que cuando le pidas que edita la imagen, simplemente se limite a editar imagen. Y si quieres una transformación completa, como pasarle múltiples referencias para que genere
una imagen nueva, también sea capaz de hacerlo. Esto es uno de los puntos principales y más atractivos de Nano Banana, que es un modelo sumamente versátil.
Y seguimos con más ejemplos. Otro caso de uso muy interesante es la posibilidad de pasar de un sketch, como puede ser este, a una imagen terminada. Simplemente pidiéndolo que nos convierta
cualquier voceto en una imagen fotorrealista, podéis ver como podemos pasar de esto a esto. Y esto puede ser muy interesante, por ejemplo, para arquitectos que tienden a dibujar
bastante a mano. Pero lo interesante es que con nanobanana podemos combinar ambos extremos de cómo podemos utilizar esta herramienta. y me explico. Podemos pasar de que nos genere algo completamente nuevo a partir de una
instrucción de texto a utilizar esta imagen que nos genera para incorporarla contextualmente dentro de una imagen real. Y lo vamos a ver con este ejemplo y quedará clarísimo de a lo que me
refiero. Por ejemplo, le podemos pedir que nos diseñe una etiqueta. Diseña una etiqueta cuadrada para una cerveza artesanal llamada Virral y automáticamente te propone una etiqueta
como esta, virral, e incluso le ha puesto un lema debajo, cerveza artasanal, aquí hay una pequeña falta, elixir del bosque, pero bueno, nada mal,
pero lo interesante es que lo que nos genera la inteligencia artificial dentro de la misma conversación o del mismo hilo lo podemos integrar en otra imagen. Por ejemplo, una imagen como esta, que sería la botella de cerveza. Aquí podéis
ver que si yo le pido simplemente que ponga la etiqueta en esta imagen, obtenemos un resultado como este, por lo que faltaría calidad, faltaría definición, quizá faltaría hacer la
imagen con mayor resolución, pero fijaros que ha conservado perfectamente el contexto de la imagen que había generado, por lo que puede ser un muy buen sustituto al uso de Mocaps también.
Y aquí atentos al siguiente caso de uso, porque esta capacidad para modificar imágenes, modificar texto y entender contexto puede ser especialmente útil en publicidad. Por ejemplo, si queremos
reinterpretar un anuncio para llevarlo a otra cultura. En este caso partimos de un anuncio como este de aquí, que es un anuncio de una lata de refrescos que
está completamente en inglés y al que yo con un promo un poquito más largo le pido que me lo rediseñe para adaptarlo al público japonés de un modo minimalista y automáticamente me
devuelve un primer resultado como este que estáis viendo aquí. ha incorporado textos en japonés, que se los he pasado chat GPT y eran coherentes con el anuncio o el producto que estábamos
anunciando y además ha incorporado incluso carácteres japoneses en la propia lata y el diseño que ha que nos ha propuesto es simplificado, es más minimalista que el diseño original, pero
una vez tenemos esto, que no ha sido un cambio muy radical respecto el original, le podemos decir que lo contextualice un poco más en la estética japonesa que nosotros queremos y le digo algo como
esto, hazlo aún más minimalista, cambia la composición mostrando el producto y ponlo en medio de un bosque de cañas de bambú húmedo y automáticamente nos devuelve algo como esto, por lo que de
un modo rapidísimo nos puede servir para contextualizar productos o anuncios en otras culturas. Y si algún detalle del diseño no nos convence, como el hecho de que las letras no se lean
suficientemente bien, se lo podemos pedir y automáticamente pues cambiará las letras de color. Este, pues sería otro caso de uso que me parece muy muy
interesante de Nanobanana porque aprovecha no solo la capacidad para generar y editar imágenes, sino también la capacidad para entender la petición y el contexto y la intención con que se lo
estamos pidiendo. Y dicho esto, en el siguiente caso de uso vamos a ver otra cosa que creo que a los diseñadores también les parecerá interesante. ¿Cómo es la capacidad para manipular texto a diferentes niveles? Le podemos hacer
escribir en cualquier tipo tipografía. Por ejemplo, le puedo pasar una imagen como esta con el texto suscríbete, que por cierto es buena idea, y pedirle algo como esto. Escribe en esta tipografía el
texto cálido y suave y automáticamente es capaz de hacerlo. Pero no solo esto, si ahora le pasamos otra imagen de referencia como una posible textura, en este caso sería la textura del abrigo y
le pedimos algo como esto, usa la textura del abrigo para convertir el texto anterior en una representación CGI3D con la tipografía la textura y volúmenes de la chaqueta. Haz que el texto esté en tonos amarillos sobre un
cielo azul. Fijaros lo que hace. nos genera un texto, un una imagen con cielo en fondo azul y con la tipografía que le habíamos pasado antes de referencia,
fijaros que es exactamente la misma, esto es lo que había generado antes, lo que nos devuelve es este texto cálido y suave. Y si la imagen la vemos un poco
desangelada, le podemos pedir, por ejemplo, que añada algunas nubes CGI con la misma texturas alrededor del texto y
lo hace sin problema. Aquí tendríamos el resultado final, por lo que nanobanana también nos ayuda a diseñar eh pequeñas piezas tipográficas, manteniendo el
texto, es decir, pudiéndole pedir exactamente lo que queremos que diga. Y como podéis ver lo hace bastante bien y comete pocos errores ortográficos y además trabajando con otras referencias
para adaptarla a cualquier contexto. Y esta es la gracia, el contexto. Podemos utilizar cualquier imagen, por ejemplo, una imagen como esta de París y llevarla
a cualquier contexto por su realista que nos parezca. Por ejemplo, lo podemos poner debajo de una cúpula bajo el mar. Aquí podéis ver como me ha dado la primera versión. Como aquí la transformación ya era un poco más
profunda, creo que se ha pasado. Le digo que intente ceñirse un poco mejor a la imagen original. Y aquí podéis ver cómo me mete la imagen original dentro de esta cúpula debajo del mar. Y la gracia
es que incluso podemos virar eh perspectivas o que nos dé diferentes puntos de vista de una misma imagen. De modo que le podemos pedir, por ejemplo, que nos dé una imagen de cómo se vería
esta torre Ifel debajo del mar si hubiese una cúpula por encima. y podemos conseguir imágenes como estas. Pero aún nos quedan casos de uso y casos de uso
muy interesantes. Antes de pasar al siguiente, simplemente recordarte que si lo que estás viendo esta herramienta Google Studio o el resto de herramientas de Google te interesan y te gustaría aprender a utilizarlas en tu día a día
para sacarle provecho, pues tengo un curso completa sobre todo este ecosistema de herramientas de inteligencia artificial de Google, es decir, el chat de Gémini, Google A Studio, Notebook LM y muchas más. Por lo
que si te interesa aprender sobre esto, te recomiendo que le eches un vistazo a mi curso. Encontrarás el enlace para acceder a toda la información en la descripción. Y dicho esto, volvemos a
los casos de uso, porque como os decía, aún hay unos cuantos y muy interesantes. En este caso, vamos a hablar de un caso de uso muy habitual del que aún no había hablado, cómo es el cambio de estilo. Y
vamos a empezar haciéndolo al revés. En vez de una foto y pasarla a un estilo concreto, vamos a una ilustración en un estilo específico, como puede ser esta que estáis viendo
ahora mismo. Y lo que le vamos a pedir es que intente imaginarse la foto de referencia de dónde habría salido esta imagen. Y aquí le pido esto. Crea la
fotografía de referencia que ha inspirado esta ilustración. me devuelve esto donde efectivamente el perro podría ser este de aquí, pero aún hay muchos elementos ilustrados y lo que le digo es
que el entorno también me lo haga fotorrealista. Y bueno, aún quedaría algún elemento en formato de ilustración, pero en dos, tres iteraciones tendríamos ya la fotografía
perfecta. Y lo interesante es que obviamente podemos hacer lo opuesto. Una vez tenemos una fotografía, como puede ser esta, pues le podemos decir que nos la pase a un estilo que nosotros
queramos. En este caso le pido lo siguiente. A partir de esta fotografía, hazme una propuesta para un diseño de tatuaje minimalista hecho solo en tinta negra. Y fijaros, pasamos de esta imagen
de aquí a esta fotografía de aquí. Y lo más interesante es que, como hemos visto antes, podemos contextualizarla, por lo que si le digo algo como esto, muéstrame cómo se vería tatuado en la parte
trasera del brazo, en la zona del triceps. Automáticamente me devuelve una imagen como esta. Igual el tatuaje aquí es un poco grande, pero como podéis ver las posibilidades son enormes y lo más
interesante son completamente inmediatas. Estáis viendo que el número de interacciones es realmente mínimo. Evidentemente están casos de uso inmediatos como es el de mejorar
imágenes antiguas o restaurarlas. Es decir, podemos una imagen que estuviese en blanco y negro como esta de aquí, pedirle que nos la devuelva en
color y automáticamente lo haría. Y además le podemos pedir que la restaure, es decir, estos defectos de la imagen original que estamos viendo, sobre todo
en las esquinas, le podemos pedir pues que los elimine y que termine de corregir el balance de blancos, por lo que podríamos obtener una imagen como esta, por lo que otro caso de uso sería
este, el colorización y restauración de imágenes antiguas. Si nos vamos al siguiente ejemplo, vamos a hacer una restauración incluso un poco más acentuada y es que podemos
edificios de la antigüedad, edificios que se muestran en ruinas, como en este caso sería el caso del coliseo, subirle una imagen como esta con su estado
actual y pedirle que nos lo restaure por completo. Y aquí podríamos ver la imagen que nos ha generado. Deberíamos comprobar que sea rigurosamente
correcta. Pero de entrada, si quisiésemos tener una primera versión del coliseo restaurado, podemos ver como nanobanana es capaz de hacerlo sin ningún tipo de problema. Y volviendo al
tema de las restauraciones de fotos antiguas, es capaz de restaurar fotos que están en un estado mucho más cuestionable, como este de aquí, donde podemos ver que hay un poco de todo.
Está el elemento donde faltan pedazos de la imagen, hay todas estas grietas manifiestas y además la imagen en blanco y negro. Y fijaros, es capaz de restaurar la imagen obviando
prácticamente por completo todas esas marcas y grietas que con otros modelos similares a veces las confundía con arrugas de la persona que había por detrás. Y aquí podemos ver que ha hecho
una restauración bastante bastante correcta. Y si bajamos un poco más, pues podemos volver a pedirle que la colorice. Todo esto en apenas dos
interacciones. Y si partimos de este detalle de que nanobanana es capaz de entender partes que faltan de una imagen, esto lo podemos llevar al extremo y también pedirle que recreágenes completas a partir de
pedacitos. Es decir, si le pasamos un pedacito de una cara, como puede ser este, donde apenas se ve el ojo y el peinado, y le decimos, "Genera una imagen de la cara completa tomada como un primer plano." Fijaros lo que es
capaz de recrear. nos mantiene perfectamente esa parte y nos genera la posible cara completa de esta chica, por lo que este sería otro caso de uso, la
posibilidad de generar imágenes completas a partir solo de un pequeño fragmento. Y otro caso de uso muy interesante es pedirle a Nano Banana que
edite una imagen, no según indicaciones concretas, sino según el propósito final. Por ejemplo, este ejemplo que estáis viendo ahí, le he proporcionado una foto mía y en vez de decirle pues
que me cambie el gerse por un traje, que mejore la iluminación y demás, lo que he hecho es pedirle directamente que adapte esta fotografía a un contexto, simplemente editándola. Le he dicho,
convierte esta foto en una foto para perfil de LinkedIn. Edita su calidad y contexto para ello y automáticamente lo que hace es vestirme en traje y ponerme en una oficina. una foto de LinkedIn,
una foto de perfil de LinkedIn estándar, pero es interesante ver como a veces no hace falta hacer indicaciones concretas, sino que le podemos pedir simplemente el objetivo final. A partir de aquí, los
casos de uso se van multiplicando, incluso la manera cómo podemos utilizarlo. Ahora me voy a ir a Twitter, donde vamos a ver algunos ejemplos más, en este caso de la comunidad. Aquí
podemos ver como por ejemplo en este caso están haciendo el ejemplo dentro de FPCK, podemos utilizar nano bananana haciendo anotaciones en la imagen. Es decir, si os habéis fijado al inicio del
vídeo, lo que hacen es lo siguiente. Ponen un colage con las diferentes indicaciones dentro de la imagen, igual que se hacía con BO3. Y nanobanana es capaz de interpretar esta instrucción en
modo de imagen para convertirla en una imagen que luego animan en vídeo. Otro caso de uso interesante que he visto en la red este. A partir de una captura de
Google Maps indican desde dónde quieren que se tome la imagen y nanobarana interpreta la ubicación y supongo que en caso de que sea suficientemente conocido, es capaz de generarte una
imagen de esa ubicación. Otro caso de uso interesante, el poder anotar dentro de una imagen y además en esta imagen combinar diferentes personajes. Fijaros
en este ejemplo. Aquí podemos ver cómo han introducido dos personajes, una un pequeño sketch de en qué posición deben estar y cómo Nano Banana también es capaz de reinterpretar esto para crear
una imagen coherente. Más ejemplos. En este ejemplo de aquí lo que vemos es como nanobanana es capaz de interpretar una imagen para sacar una
imagen del modelo 3D equivalente de este barco. Y esto es muy interesante porque los modelos de generación de 3D también están evolucionando, por lo que si combinamos este barco con la capacidad para sacar diferentes vistas que hemos
mostrado anteriormente de nanobanana junto con un modelo que es capaz de generar modelos 3D, pues podríamos llegar a obtener un modelado muy rápido de un objeto a partir de una imagen
completamente contextualizada como es esta. Otro ejemplo, este es un ejemplo que ya hemos visto, pero lo pongo para que veáis la capacidad que tiene Nano Banana
para gestionar referencias. En este caso, Travis Davids nos dice que ha batido su nuevo récord pudiendo llegar a mezclar 13 imágenes en una sola imagen. Aquí podéis ver el colage de imágenes
que ha utilizado y arriba la imagen generada, por lo que podéis jugar a ver si las encontráis todas. Y con esto llegaríamos al final de este vídeo. Como podéis ver, las posibilidades cabrano
banana son enormes. No solo porque es un modelo capaz de generar imágenes, no solo porque es un modelo capaz de editar imágenes, sino también porque es un modelo capaz de entender el contexto y
la intención con la que le pedimos algo, por lo que lo que se puede llegar a hacer es increíble. Y lo mejor de todo, puedes usarlo completamente gratis. Si quieres saber más acerca de este modelo,


 
FuturePedia
I have this image of me, but I want to look like I'm having a cool night out. Perfect. That looks like a typical Tuesday for me, but I could use some company. Boom. Now, that's more like a
typical Wednesday for me. I kind of want to flex on Instagram. So, there it is. Hanging out on my yacht. But let's add in some of the homies. Bezos, Zuckerberg, Ronaldo, Swift, Beyonce, and
LeBron. Perfect. Got the whole crew together. So, that was easy and fun. But what about something practical I could use for my business? So, I think with Futuredia, we need to get into the
energy drink game. So, I dragged in our logo and it created this future fuel max energy for the AI era. So, that is impressive. But the real cool part is
this unlocks infinite product photos. Or how about using it creatively? I have this scene and I can get alternate camera angles, insert shots, and emotions all while retaining character
and style consistency. That's just a little taste. I'll cover many more use cases for fun, business, creative, tool combinations, and everything in between. The best part is you can do this all for
free right now. I'll show a few different places to use it. And for anyone that doesn't know, I'm talking about Nano Banana from Google, officially known as Gemini 2.5/image, but I'm still calling it by its former
code name, Nano Banana, as I hope everyone else continues to do forever. that has topped the AI image editing leaderboard by a wide margin and it is welld deserved. So, let's get into it.
The first place I'll show this is right in the Gemini app. You can sign up for the free plan and start using it right away. This isn't a standalone image model. It's part of Gemini 2.5. That gives it very good understanding of the
world and also natural language. So, that means you can get a lot from very simple prompts. Of course, being more specific will guide it further, but we'll start simple. Just drag in a photo of me and I will say create a Time
magazine cover with me as the person of the year. I should be wearing a suit and look as if it was taken in a studio portrait photo shoot. Additional text that says Kevin Hudson, the man who changed everything. And that is
absolutely perfect on the first try. Got all the text right. It even has the time letters behind me. A little looked just like a time cover. So, how about now make a professional head shot for me. I
should be wearing something nicer and look like it was taken in a nice studio. All right, that looks great. Now give me long hair. Perfect. So, I used to have long hair and this is actually pretty
accurate to what it used to look like. But now, give me a tattoo across my forehead that says futureedia and a tattoo on my cheek that says AI. Perfect. So, these collaborative edits through natural language are one of the
things that make this so useful. ChatGBT and Flux Context both have this same ability, but Nano Banana is just much better at it. Now, there's obviously endless types of edits you could make like these. You can see how you look
with different hairstyles, eye color, facial hair, glasses, on and on. But let's try something harder. Create a movie poster using my face. It will be a parody of the good, the bad, and the ugly called the good, the bad, and the
AI. It should have a western theme, but with an AI crossover twist. Maybe a robot as one of the other characters or something. As I said, it understands natural language pretty well. I'll just let it figure that out. And that is a
really good movie poster, but it doesn't really look like me. And something I found when this happens is re-uploading the photo and saying to fix it just doesn't really work. It tends to just give back the exact same image. So, I don't know, for some reason, maybe it
just thinks this looks like me, I guess. But I'll show some of the workarounds for this and other issues as we go through. But the easiest way around it in this case is to just use the same prompt again and let it start from scratch. And there we go. That looks
much more like me. That's a really good movie poster. Even used a nice unique font. The good, the bad, and the AI. The man with no server. Three icons, one final code. No mercy. I don't know that
I follow that tagline. And all this text at the bottom is just gibberish, but that is super impressive. These types of edits are a very common way to use this. So, I'll just jump through some others like this from the community real quick.
So, they had some of these like trying on the different outfits of a giant dog rising out of the sea. Make a miniature model of the lighthouse and put it on this table. That's solid. We'll do more stuff like that later. Another one with
a jacket and shorts. We're going to put this shirt on him. And it looks like that. That's perfect. You're changing the background and the clothing. That looks just perfect. He kept the same pose perfectly. It even figured out how
to do it around his arms folded up like this. That looks great. Chris Castanova did the movie poster thing, too, but she included seven people on it. Looks like it nailed all of that. And you can get pretty crazy with these references.
Travis Davids really pushed this and combined 10 things all in one image. You can't actually upload that many references. So what he did was made a collage of each of the different items and just added a label to each item and
it got every single one of these with all the finer details all into this image. He post another one pushing that even further up to 13 elements. It's hard to check for every single detail,
but it looks basically perfect. So we'll move on from there. Now people in the AI world have been throwing around the idea that Photoshop is dead because of this. Not the first time that phrase has been thrown around, but let's test out some
of the specific things you would typically do in Photoshop. We'll see how right they are about it this time. This time I'll actually switch over to a different platform to use this on. Google AI Studio. This is a platform
where you can use a lot of Google's newest models all for free. I've actually made an entire video just on this platform. There are limitations to the image and video generations, although I haven't hit the limit on image edits yet and I've been doing a
lot. And now upload an image. Now, this is a photo I took, but I decided to just mess up all the colors and brightness on it. Now, I'll just say fix the color in this photo. This is what came back. That
looks amazing. I think I actually might like it more than the way I color corrected my original. So, great job there. So, I'll try something with this photo I took. Then I'll upload an image
of me and say, "Add me into the photo leaning up against the side of the van." And that looks good. The face is a little warped, though. But now, let's add some snow on the ground. Got that perfect. How about give me a sweater?
That's something I'd usually use generative fill for right now, but it does a good job with that. Now, let's try to add some text. So add the word Aurora across the top in a cool font that matches the vibe. Okay, that looks
amazing. That was perfect. It's a really solid font and even has a nice little glow to it. So I would say it nailed that task other than the face not being clear. I'd have to fix that another way. One option would be with Magnific. That
would work pretty quick to get a nice face on there. I will cover some of the upscalers later. The images you get back from Nano Banana are pretty low fidelity, especially when you're using them in the Gemini app. So using an
upscaler after you're done in here is a very good idea. Let's try some more adjustments. So I've got this photo of a moose I took while hiking. Change the leaves to fall time colors of red, orange, and yellow. That looks good.
Mostly red and orange. Not really yellow, but that's fine. It changed the color of the grass, too, though. That's blurry in a lot of areas. We'll see if it can distinguish what's grass and what's leaves. Okay, that did a really
good job with that. But now you might be thinking, "Hey, it's fall time. The moose would be shedding its velvet by now." So I'll see how well it understands this. Now, the moose shouldn't have velvet on its antlers since it's fall time. Remove the velvet
so they're hardened antlers during the rut. Okay. Wow, it actually nailed that. That's amazing. One more common one is removing people or objects from a photo. So, I've got a
really difficult one of the sunrise through Mesa Arch. There's just tons of people here and it got rid of them all really well. That looks great. This is the photo I took from within that crowd, by the way. Definitely one of those
Instagram verse reality type places. But here's another thing it's really good at colorizing images. So here's the Hindenburg. It did an incredible job with that. Now, how about this famous
Tesla image? Yeah, that looks solid. It still kept the old school feel to it. Now, it does work for restoring damaged photos as well, but only to a point. So, I found this really bad one, and it imagined new people that are clearly not
at all like the ones from the original. Like, it looks like that's an older woman, but turned into a young boy. So, there's definitely a limit there. Same kind of thing with blurry photos. It works for a little blurry, not super blurry, and sometimes it'll like just
reimagine the photo. So, I did this one of a black bear that I zoomed in a lot on. I asked it to add detail. It came back and looked pretty good, but it changed the position of the bear. That makes it feel like it wasn't an authentic photo. So, I would never
actually use that. I was hoping it would just sharpen it up and keep everything else the same. But, I'm not the first to try this. Here's a bunch from Rodrigo Brezane. All right, this is a really long thread.
I won't go through all of these, but you can see it does an just incredible job with this. Probably one of the best ways to colorize photos. Now, this image editing is an impressive feature, but
it's only a small part of Gemini. So, a great way to level up with the other aspects of Gemini is with this free resource provided by HubSpot. It's called Google Gemini at Work, and it's a
complete breakdown of how to use Gemini to speed up your research, improve your content, and build entire marketing strategies in a fraction of the time. My favorite part is the Gemini marketing
stack. It shows how five tools like Deep Research, Notebook LM, and Gemini 2.5 Pro work together to handle everything from campaign planning to content creation to building interactive
dashboards, all without needing a big team or agency. There's also a 4-week rollout plan if you want to actually start using Gemini in your workflow right now, plus some copypaste prompt templates you can drop into the chat
right away. If you're in marketing or just curious how far you can push AI tools, you'll definitely want to grab this one. The link to download it is in the description. Thanks to HubSpot for sponsoring this video and making these
free resources available to everyone watching. So, it can do a lot, but let's get into something I use Photoshop for. Making thumbnails. It's definitely a harder task, so this will highlight some of the limitations. Now, you may have
seen people on Twitter hype it up for this specific thing, but when using it in practice, you run into a lot of issues. And I'm back in Gemini for this one, by the way. Make me a Mr. beast style thumbnail with my face on one
side, my mouth open, and my hands on my cheeks with a shocked expression. On the other side is a computer. On the screen is a banana with text underneath that says nano banana. The background is futuristic. And this is the result I
got. So, it's not bad. It was definitely impressive that it can do that, but the style is pretty old school. I'd want to make some changes to it. I could see the style working in some niches. Let's do another one first. Give me a thumbnail
in the style of a tech software reviewer. Use my face and make it a thumbnail for a chattrial. basic prompt. This is the result. Definitely not good. I mean, it followed the prompt, but it's just not a good
thumbnail. We'll do one more. So, I'm going to try to recreate a specific thumbnail I've used in a previous video. So, I've just got a little description of what I want to basically just recreate this thumbnail. And right here, we come across a very common issue in
here is the aspect ratio. It's not very good at following what you say. It defaults to whatever aspect ratio your image was. And I used two images in this one. The NAND logo was a square, though,
so I decided to make the whole thing a square. And if you ask it to change the aspect ratio to 16:9 or 916 or anything else, it won't do it. It'll just give you back the exact same image. Maybe
they'll fix that someday, but right now it's not even worth trying. It has never gotten it right for me. So instead, I just took that NADN logo and extended it out, just adding blank space on the sides. So now both the images I uploaded
were in 169. Now it came back with this one. You know, it's fine, but it forgot the text. So I asked for the text banner, you know, added a plain banner on the bottom. So, I asked for it to move that up to the top right, but it
left the pink on the bottom. So, I said, "Now remove the pink from the bottom." And it was still there. So, I asked again in different words. Didn't work. So, I changed the wording again. Still
did not work. My apologies for the oversight. I will remove the pink banner from the bottom of the image now, but it is still there. You will definitely have this happen where it just gets stuck on something like it'll be able to do these
really complex amazing things, but then something super simple like remove this little line at the bottom, it just fails completely. But with something like a thumbnail, you need to make small changes all the time, like lots of them.
So, I've found that this does not work well in practice when you need a bunch of small changes in a row. I also tried one where I drew out the whole composition and asked for it to create the thumbnail from that. So, I did a
good job and made a couple changes, but it got stuck again. But to fix it, it's way faster for me to just pull it into Photoshop to make these changes. So, what you could do is get close, then ask it to isolate the different layers. Like
maybe in a new chat, say remove everything but the text. Then remove everything but the background and remove everything but the person. Then you'd have these different layers you could bring into Photoshop to work from. And I
will say these are just some quick tests I did. But if you have a concept and sketch it out really well, have some good references and a really solid prompt, I think you could get some good thumbnails with this, or at least close.
But in real production for most people, you'll probably still need to pull this into Photoshop to make further edits. And that's not just for thumbnails. Same goes with like graphic design, photo edits, web design, and lots of other
work. This does not make Photoshop obsolete, but it is a great tool to add into your toolkit in a production pipeline. Let's get back to some business use cases. I'll go a little more over those product photos I made at
the beginning. It is so easy to do, and I didn't even show all the ones I made. To create the product, I just gave a simple description. Give me a sleek energy drink can with this logo on it. and it's called Future Fuel. And that
looked just amazing right out of the gate. I did ask it to change the slogan to energy for the AI era. And got that perfect. The first thing I did was take that bar image it made and ask it to replace the drinks, but it only got one
of the drinks correct. So, I asked it to replace the other. Just removed it. Asked it again, put the original drink back, asked another time, and it cropped it into a square, but it did add the
drink to her hand. This is what I say when I ask it to go back to 16:9. It just does not work. So, for me, it's way easier to just go fix that in Photoshop. I just layered it on to the original. Then I said, "Fixed it for you." And I
wanted to have them drinking from the cans. Now, then she decided to drink from her secret can that was hiding behind me, but got it all to work in the end. Now, for all the other product shots, I did some by adding in a prompt,
like this one right here. Add a pretty long prompt. Came back just perfect on that one. Looks amazing. Then, some of these other options I tried just weren't looking too great. It did its job, but the images just weren't that good. So
instead, I went into Midjourney and downloaded a bunch of images from there. The midjourney aesthetics just can't be beat. So I would take those images and just ask it to replace the can. And of course, I ran into the issue of the
different aspect ratios. So taking my original square can image, I just cropped it also into a vertical and horizontal aspect ratio and just matched it with the image I was uploading. You can see as I'm scrolling through these,
they almost all worked on the very first try. It is just so good at this. So for product placement and making ads, this is an amazing technique. And since you can include text in your images as well,
there's a lot of additional use cases that open up from this. So just sticking with this future fuel example, let's test out some other stuff. It's kind of funny when I asked it to create a landing page, but it added all this other additional design work for the
landing page here. But this is the visual representation it added at the bottom. Looks pretty good. There are a couple misspellings on here, but that looks decent. I asked it to give me an alternate option. I do like this one a
bit more. I tried one more time and I think this third option was the best of all. That is a very solid landing page. And since this is Gemini, you could actually have it code this for you right in here. But we can do more than just
landing pages. How about some sort of data visualization? I wasn't sure exactly what to use, so I let it come up with it. Said come up with a data and visualization idea yourself. Feel free to use madeup data and numbers. Had a
big long answer here, and this is what it came back with. The future fuel's impact on productivity and focus. has a really nice graph here on how many hours into your workday before you crash. I had to make me some business cards. Then
I decided to try one that seems a bit harder than that to create a full font for future fuel, including every letter, but using the font on the can as inspiration. And this is the font it gave me. Then I guess if you take the
top line, the N from this line, and then this bottom line, that's the whole alphabet. All this other stuff is just extra here. But this does look all very cohesive and a pretty solid font. I think these little lines through them
are based off of the F on this logo here. Trying to match that. There are just tons and tons of different ways you could use this in all aspects of your business. I tested a ton of just random silly stuff. We'll get into that for
just a second here, too. So, I asked for me as a South Park character. I mean, that just looks perfect. It's exactly how I would envision it. And as a Minecraft character, that one looked all right. Yes, I can see it. And there's a
rubber hose animated character. That one looks really good. But I kept trying to match the same setting, so I asked for it to do a GTA 5 character walking down the street. And yeah, I'd say it nailed that. I don't really play any video
games, but I think it did. We got a claimation character walking in a forest. An action figure in a box. The dude action figure. Grizzly guy, I guess. Okay. For ages eight and up. I I
don't know. Definitely looks like me, though. I saw someone do this one. Show me what my dog would look like as a person. The dude definitely looks stoked. and he was wearing his little blue vest that day. Then one more was turn this image into a black and white
coloring book page. Did really well with most of that. There's just a ton of fun stuff you can do in here. I tried to have it create a meme for me. Just uploaded the Photoshop logo, my face, and the Drake meme. It didn't replace my
face on here, so I uploaded this new image and my face again, and it did work. Not bad. I wish it had merged it a little bit better with him, but looks good. Here's another one that ended up being just super fun. I took some old
random drawings I had from like back in high school. I think I used to just doodle random weird stuff. So I started uploading those and said give a real world representation of this and I love what it came back with. I don't think
you could have done that better. Like to go from this little drawing to that is just awesome. Then the next one I had this guy just yeah weird random guy I drew and it came out like this. That is
just incredible. I did not expect it to turn out this cool. Some of the others weren't as good. So it was hit and miss on these. This one just kind of added color to it. But yeah, that looks pretty cool. This one looked pretty good. And
hey, this is High School. Drew some weird stuff. Somehow it made that work. The weird little conjoined twins here turned into this. That one is just awesome. I mean, I'd watch that show
tonight. Also, I kept trying to have it do this one right here. This kind of weird flower thing. And I would say, is there another image I can try? I'm here to help create all types of things, but can't make images like that. No idea
why. But that brings me to the other place I use this a lot. Freepick. So, I like Freepick. It's one of those platforms that has just all different types of AI tools, image generators, video generators, image editors,
upscalers, just everything all built in one. So, you don't have to have 20 different subscriptions. You can just use them all in one place. Currently, if you're on one of their higher plans, you get unlimited Nano Banana. Another benefit of using it in here is it
doesn't have the watermark and it doesn't compress the image as much. It is a good option. It won't be free like the others though. But, when I brought that in here, it worked and I really really liked this one. So, that's just like a random side one I thought was
pretty fun. Now, let's get into some of the more less obvious and complex use cases for this. Eccentrism Art made a nice little comic here as consistent characters across each of these frames. I really liked this one from Albert
Einstein. The prompt was extrude this handdrawn fantasy map into a complete navigable 3D tabletop world, interpreting contour lines for elevation and symbols for structures like castles and villages. So, it went from this map
to this tabletop world. Thought that was a very creative way to use this. I like that a lot. This one, isolating a single building or object and turning it into an isometric model. Just a very simple
prompt to make an isometric model of the object only. Even from this just kind of terrible picture here, it was able to make this model. There's so many little details in here. It's very intricate.
And as they show in this comment right over here, you could bring this into an image to 3D generator and turn it into a 3D model. There's a few different sites you could do this on. There's Hunuan, there's Trippo, and a couple others.
They had a whole string of examples here. These just look amazing. Just being able to extract that and turn essentially anything into a 3D model. That's just incredible. That used to be a very complicated pipeline, but now
super easy. This is kind of a variation on how I was turning my sketches into a real life image. So Brent Lynch used the prompt depict as a live big budget costume test on set shot on film. This
little cartoon into this guy. Jared Lou has this one with all these annotations similar to how I made that thumbnail earlier where I sketched all over it. So he brings on each of these people and the plant and then adds a caption for
each of them to explain what they are. Just drops that in using freepic and he shows the video here, but down here is the image that came back. He understood exactly what he was saying and generated the rest of the body for each of the
people. Then he just turned that into a video. The fact that you can just mark specific areas and provide reference instructions is really powerful. Fact that it actually understands what you're intending to do. There's so many
directions you can take this in and use cases for it. It just gives you a lot more control. So this is another example using that same technique. Gave specific instructions on each different part of the building. And then this was the
final result. It followed that so well. It'd be very very difficult to try to explain that in a text prompt. Really the only way you're going to get that is by annotating the image. This one from Balo is really cool. So since Nano
Banana has Gemini's world knowledge, you can just upload screenshots of the real world and ask it to annotate stuff for you. His prompt was, "You are a location-based AR experience generator. highlight point of interest in this
image and annotate relevant information about it. Have to double check if this stuff is accurate. I would guess that it is in that same kind of realm with using Google Maps as part of the process. Using the prompt, what does the red
arrow see? And just drawing right on a Google map, showing the perspective as if it was in the position of that arrow. There's quite a few of these. What does the red arrow see of Tokyo Tower? It
says it looks like this from there. Now, I'm going to go ahead and assume that these are not accurate and it wouldn't look exactly like this from that position, but just from this, it understands what that's asking for and to be kind of sitting on the water
looking at the Golden Gate Bridge. That's some pretty incredible understanding. Another amazing example here from Will Borsen. You can spend hours fiddling with lighting, materials, environment variables in Keyshot or
Blender, or you can send a screenshot of CAD to the image model with a one-s sentence instruction. It's got a CAD screenshot of an aluminum extrusion frame. Give it a realistic clear anodized aluminum finish and give it a
black studio photography. And that looks like it got it exactly right. Give it a white background. That one does look a little better. There's so many different applications across various industries for this. Of course, there's all the
interior design and real estate uses like testing out different wallpapers on here. There's just always such an issue when trying to use ChachiBT for this. It would always change stuff about your image. It just never looked right. This
keeps all of the other areas of the image so intact. So you can really get an idea of what this will look like. That is just such a useful thing to do with Nano Banana. Then a lot of people have been using this to create
consistent characters across scenes. It's far better at this than Flex Context or ChachiPT. This one's from Jared Lou. Like this girl looks the exact same from shot to shot. Perfectly
consistent. And Metapet actually made a short little video doing this. This is all based off of one image. It just maintained that consistency as he mentions here across all these small details. and he said to pay attention to
his shirt and hat in particular as you're watching this. He said this is the original image he used and then prompted for him in different situations. All of these and I showed some examples of myself testing that out at the beginning of this video. So this
is my original image just this little anime scene in a market. I asked to move the camera up and got a higher angle shot. The perspective looks a little weird but not too bad. Then a close-up of her, close-up of him. Closeup of the
cat, of course, a shot of the overall market. Then an insert shot of someone just picking up an apple. an aerial view of the market. Another higher angle. Then I tried to zoom out on that higher angle I had. Then I asked for her giving
all these different emotions. I had a few variations of this and picked the best. Did the same thing for the guy and the cat and randomly gave me just a normal cat. But you can quickly get your
character across any scene, maintaining the style and even small details of that character. This is by far the best form of character consistency we've had. Something we've been waiting for such a long time for. Now, once you start
combining this with the video tools, it unlocks unlimited possibilities. So, I tested out just a couple really quick ones. So, this is a photo I took of this rock with the Milky Way above it. And
then I just asked to switch that to daytime. Then, I brought that into Cling and asked for a time-lapse. And that looks just so good. This is using their new start and end frame feature. You know, I would never actually take a real photo and do this with it, but if you're
adding something into like a shot in a movie or an AI film, this is perfect for that. Then of course I use that image of me and the woman in the bar and animated that.
Another way to use this is to get B-roll for videos. Say I mentioned working in a coffee shop and ask Clling for a quick video. The physics are not perfect on this one. This is not a great example. I know the coffee cup just appears from
the side there, but that was just a quick demo of what you could do with it. Runways act 2 is another way to use that. I can just take any still from this video and then bring it into Nano Banana. ask for a different style or
character, then bring that into runways act two, then it will keep the hand movements and the lip-s syncing consistent through the whole shot. Now, I did mention before these come back in really low resolution, especially in the Gemini app, so it's a very good idea to
use an upscaler. There's lots of options for this. My primary upscaler is magnificer that keeps all the details intact. Then they also have their creative upscaler where it's a generative upscale that
reimagines your image, keeping the structure intact while adding details. So that's the main one I use all the time. I have no idea how many use cases I covered here. And even with all of that, I feel like it still barely
scratches the surface of what's possible with Nano Banana. Like this is actually a very big leap forward in AI. It's been a ton of fun to play with. I will spend a lot more time experimenting. But I hope that shows you what's possible and
what isn't. But if you want to go much deeper on learning AI on Futuripedia, we have a full course platform with over 500 lessons across over 20 AI courses. You'll find full learning paths on
chatbt, prompt engineering, custom GPTs, video generation, coding with AI, and a lot more. It's all included in one subscription. You can get a 7-day free trial using the link in the description,
or check out this video on Google AI Studio. I do a whole deep dive into all the other things you can do in here completely for free with a lot of features 

Ishan Sharma
I don't say this often, but this might be the end of Photoshop and Canva. Hi everyone, I'm Isan Sharma, and Google just unveiled Nano Banana. Its state-of-the-art image generation model
that is blowing everyone's mind on the internet. Nano Banana, also known as Gemini 2.5 flash image, retains character and scene consistency while allowing you to change anything about
the image that you want by simply typing to it in plain English. In this video, I'll show you how to use Nano Banana for completely free. I'll show you the craziest use cases of Nano Banana. And
at the very end, I'll show you the biggest business opportunities with Google's latest image genen model. Make sure that you watch till the end. Hit the like button and subscribe. And let's
get into Nano Banana. Okay, so this is Nano Banana by Google available to you in Gemini and Google AI Studio. The way it works is that you give it an image, you give it a prompt, and it completely
changes the look of the image by just describing what you want to be changed. As you can see right here, you can also give it two images. You can ask it to combine the image. Let's say both of them are cuddling. And then you see a
picture of the dog and the girl cuddling in one image itself. You can ask it to change the interiors of a room that you want by adding elements. And it does it seamlessly as you can see right here. Now you can go to Google AI studio and
go to build and this is where you can see all the apps that Google has made using the latest nano banana model. For example, we go to fast forward. This is a free app that they've made in which
you simply upload an image of yourself. So I'll just say upload photo. Let's say I want to upload this image of mine and I click on generate. It will basically create images of mine from the past
decades. So, how I would look like in 1970s, 2000s, 1990s, and so on and so forth. And as you can see, this is all the pictures of me using Nanobanana in all of the decades. I don't know which
one you like the most, but I particularly like the one in 1980s. Let me open this up. Okay, as you can see, it retains my face. It adds the outfit.
It adds all the background. And it makes it look very real. I don't think you can tell that this is fake. You also have pix shop in which you can upload a image of yourself and do all types of retouching and professional editing that
you want to. So I'll put an image of myself. I can crop it if I want to do that. I can put some filters. Let's say I want to make myself anime. I can apply this filter and it will turn me into
anime. And it turned me into an anime. That is the power of nanobana. But this is just one way to use it. The real way to use it is by literally just going to Gemini and just importing an image. So you can
choose the tool to be image and just drop an image of yourself and by simply describing the changes you want in English, you can make any change to it that you want. First of all, you can add
text. So let's say we can say add text on the navy shirt he is wearing. Cool. Let's just say we say that with a
logo of Google for example. and it's going to add that on the image itself. And it's super fast. That's another great
advantage of having Nano Banana is that every change that it does, you can see it in about 5 to 10 seconds. As you can see, the image is already ready as I'm speaking to you and it is good to go. As
you can see, it added that this text of cool along with the Google logo. There's a lot more that you can do. You can, let's just say, change his hairstyle
to that of men in 1970s. And it's going to change the hairstyle of mine while retaining everything else as it was before.
As you can see, it's already generating the image. And I can keep describing more changes to it. As you can see, it changed my hairstyle right here. I can also add or remove items that I want. So
let's say I say remove the watch that he is wearing on both his hands. There you go. Both the watches are gone and it retains
everything else like we had before. I can keep adding more stuff to it. Add Elon Musk behind him standing and resting his
hands on the shoulder of the guy. Let's just do that. It's going to put an image of Elon Musk standing over here resting his hands on my shoulder. Now, you can also swap
faces that you want. Let's just say I want to swap my face with that of There you go. That is Elon Musk.
Swap the face of the big hair guy with Donald Trump. See, I want to do that. See, you can even make mistakes with the spelling and it will still understand
what you are trying to communicate because it is built on the 2.5 flash model by Gemini. And our image is almost ready for us to see. And there you go. It swaps faces as well. And it retains
everything else. Let's just say change the outfit of the guy sitting and make it
old money style. You can change outfits as well. What you can also do is you can literally go to Zara. Let's just say I want to try on an
outfit. So, we go to Zara. We go on to the best sellers. Let's just say I want to see how I will look if I will wear
this jacket. So, I'll take this jacket. I will basically click a picture of this And I will add it here.
Make the sitting guy wear this jacket. And it's going to now change that outfit. As you can see, this is a
old money outfit according to it. And apparently Donald Trump is still wearing glasses because I was wearing glasses before. So there is that. And you can literally just add any outfit that you
want to yourself by just describing to it in English. How cool is that? This has never been possible before. Yes, you can add stuff before, but it also changes other parts of the image. As you can see, the the coat is gone and the
jacket is on. And you can just keep telling it to do more and more things that you want. It is so insane. Like the final image is this. And we started with this image of myself. You can even ask
it to show you different angles of this same picture. So, show me the side angle of this image. And there you go. This is what the side angle would look like. But
we're just scratching the surface. You can also use Nano Banana for marketing and creating ads. For example, I have this product which is iPhone 16 Pro Max. And I can ask it to create a banner ad
and show me how that looks. Show me the banner ad of this iPhone 16 Pro Max
near the airport. and also add a very powerful oneliner slogan
for the Apple iPhone. Take the Mumbai airport road for example and it's going to make a image of a massive banner ad in which
this iPhone is visible and it can also put some ad copy of it. Now imagine that you have a product. You can literally put an image of your product on the banner. You can put it in that view and
you can see how it looks like. As you can see there you go. And as you can see, it added not just the product itself, it even added the Apple logo,
the exact typography of iPhone 16 Pro Max. It added a slogan capture tomorrow today and it gave it the banner ad aesthetic and it put it in the Mumbai
airport road itself. How crazy is that? And you can keep creating more and more ads. Create a bus stop ad with this iPhone
16 Pro Max image and add a quirky oneliner slogan and it's going to make a simple
bus stop ad where people are standing, they are waiting and so they can see the ad right next to the bus stop. And there you go. This is what a bus stop ad for iPhone 16 Pro Max would look like.
Warning may cause extreme photo envy. As you can see, it's really smart at understanding text, understanding the visuals, and putting it exactly where it needs to be. And you can also turn any
image into an ad that you want. So, for example, turn this image into a Bumble ad with a oneliner slogan
and the branding and colors of Bumble. So, we can basically turn this image into an ad in seconds using Gemini's 2.5
flash image. And there you go. This is how it looks like. You can even create entire YouTube thumbnails by just using Nano Banana. And let me show you how. So this right here is a picture of myself
with a laptop. And I can add this prompt which says make a YouTube thumbnail. Dimensions we don't need to specify. Put the guy with the laptop at the center. Replace the background. Put a coding related background. Put logos of Python
programming language. JavaScript C++ around the guy. Make some golden light come out of the laptop screen that the guy is looking at. Put text. Start coding.
now at the center bottom and I can literally just press enter and it would create an entire YouTube thumbnail for me. This is huge. This was never possible before. I've tried Flux. I've
tried all the other image generation models and this was always a big problem. The only limitation with this model, as you can see, this is what the thumbnail looks like. So it has the
logos, it has the person, it has the golden light coming, it has the coding background, it got the text right as well. And it just looks great. The only limitation is that the resolution
dimensions of the output image will be the same as that of the input image. You can even ask it to swap faces in any thumbnail that you like. So for example, you go to YouTube, you like a particular
thumbnail, download that thumbnail and say remove the guy in the right from this
image and it will remove this person from this thumbnail itself and then I can insert myself into this thumbnail and it will look very real. Let me just show you how good it looks. And as you
can see the person over here is gone. Now I can input my own image and it will basically put me in this image. And there you go. This is what it looks like. You can always ask it uh make the
guy on the right really big. It will make the person bigger. But the point here is that it is now possible to create entire YouTube thumbnails by just describing what you
want to it. There has never been a better time to become a prompt engineer because you need to know exactly what to tell it so that you get the best outputs from these models. And there is still a
lot more that you can do. So imagine that you are viewing maps of any place and you want to see exactly what this monument looks like in real time. You can put a image of this map. For
example, I'll put an image of let's just say I want to put this image and I want to see exactly what cink looks like from the perspective of this arrow. So, I'll
add this and I'll say draw a ground view picture from the arrow and show me what it looks
like. Now, because it is using the world knowledge of Gemini 2.5 Flash, it knows exactly what this place would look like and it can draw a picture of that and
show it to me in real time. And this is the image that I got in return. This is the view from Dada of Sealink. How cool is that? You can pretty much just go to
maps. Let's say I'll go to maps. I want to go to London and I want to see what does the Tower
Bridge look like. So, I can go onto Tower Bridge. I can click a screenshot of this and I can plot an arrow and ask it to show me what this place looks like. And
then I can go into Gemini. I can upload this image. I can go onto images and I can say, "Show me the ground
view from the perspective of the a arrow. There you go. This is what it would look like from that view itself.
How cool is that? I also asked it to draw the ground view of Gateway of India, Mumbai and it gave me this image right here as you can see over here itself. So this is a really powerful
model that can give you images of pretty much anything that you can imagine. You can even take multiple images and drop it into one big image like so. So we have some outfits, we have a dog, we
have some glasses, we have a car, we have headphones, we have a parrot. And we can literally integrate all of these images into one image itself like we have over here. So I just said take all
the elements and craft one image with it. As you can see, it made this image right here which has all of the elements. And notice how polished it all looks. It includes the outfit, all the
other elements, the person, the car, everything. Now, here comes the big opportunity with Nano Banana. Imagine creating an application which is powered by Gemini 2.5 flash image model and
creating something that adds value to people's lives. For example, giving them a particular style of their images, helping them remove particular elements in their images, helping them create new
variations of their images. Create an app that makes it really in intuitive for people to just tap buttons and change anything that they want in the image. Create a virtual try- on clothing
app in which people can literally drag and drop outfits they see on H&M or on Instagram and they can add it to themselves and see how they will look if they are wearing that outfit. You can
literally just start building anything that you want. You can go on to build and you can say create a virtual tryon app using Gemini
2.5 flash image where user can drop an image of the outfit and it a image of
their own and it shows how they would look if they buy this outfit. Press on enter and it will create this app in front of you.
It's going to start writing the code in front of you and in about a few minutes this app will be ready for you to preview. Now the coolest part is imagine if they try out this outfit using your
app. You can have a link for them to buy this outfit on your app itself and you can start earning affiliate commissions. Okay, let's try to upload the image of myself and I can upload an image of an
outfit. So, let's go on to Zara once again and I will choose this jacket for myself.
I think I can go onto this screenshot and I will just drop it over here. So, I have this leather jacket and I will
virtually try on the jacket right here and see how it looks like. And there you go. This is the image of me trying out this outfit. Now, you can do a lot more with this. You can allow people to download this image. You can allow
people to further make some changes to this image. You can also allow them to upload more outfits that they want to try on top of it. You can also just launch this app. So if you want to
deploy this app, you can go to deploy to cloud run. And you can create a real app that people can use with this link itself. You can also share this app. So I can click on save. And I will now be
able to share this link of this app. I'll put it in the description of this video. If you want to try it out, you can try out and let me know what you think of it. But in less than like 5 minutes, I was able to build an app
which looks so good. It works so well. I can obviously make more changes to the UI of it, but honestly, this is the best opportunity for you to make an app that millions of people can use every single
day to make their lives better and for them to edit images more effectively. And Google's nano banana is just the starting of what the future of image editing would look like. It would only improve from here. So it makes a lot of
sense for you to figure out a business idea that uses the API of Gemini 2.5 flash image. You can now access Nano Banana on the API and create powerful
apps that you've never done before. You can just go to AI Studio which has direct access of 2.5 flash image model and start using it to build any app that you want to and see how it looks. If it
works well, then you can go on to lovable or replet and try building a full end to-end app by just wipe coding and launching it for your users to use. It's all about how creative you can be,
what use case you can find for people to use this image editing software and launch it and grow it into a real money-making business. That is all from me today. Thank you so much for watching
till the very end. This was Google's Nano Banana. Have you tried it yourself? Let me know below your experience and I will see you all in the next video. Hit the like button, subscribe, share this with a friend. Let me know if you have
any questions below in the comments. I'll see you in the next one. Bye.
